<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><meta name=description content><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400&display=swap" rel=stylesheet media=print type=text/css onload='this.media="all"'><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]},loader:{load:["ui/safe"]}}</script><title>A pragmatic frame for AI Ethics</title><link rel=canonical href=https://federicotorrielli.github.io/blog/posts/ai_ethics/><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;line-height:160%;color:#1d1313;max-width:700px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3{font-family:old standard tt,serif}strong{font-weight:600}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-size:35px}h2{font-size:28px}h3{font-size:22px;margin-top:18px}h1 a,h2 a,h3 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,.date{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content .date{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#links{display:flex;justify-content:space-between;margin:50px 0 0}#links :nth-child(1){margin-right:.5em}#links :nth-child(2){margin-left:.5em}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}@media(prefers-color-scheme:dark){*,#nav h1 a{color:#fdfdfd}body{background:#121212}pre,code{background-color:#262626}#sub-header,.date{color:#bababa}hr{background:#ebebeb}}</style></head><body><section id=nav><h1><a class=h-card href=https://federicotorrielli.github.io/blog/>Federico Torrielli - Blog</a></h1><ul><li><a href=https://federicotorrielli.github.io/blog/>Home</a></li><li><a href=https://github.com/federicotorrielli>GitHub</a></li><li><a href=https://evilscript.eu>Website</a></li></ul></section></body></html><section id=content><h1>A pragmatic frame for AI Ethics</h1><div id=sub-header>June 2025 Â· 3 minute read</div><div class=entry-content><p>The phrase &ldquo;AI ethics&rdquo; is often a undefined set of slogans, compliance checklists, and moral intuitions. That sprawl hides a simpler argument: alignment is a control problem under uncertainty. While I cannot say I am an expert in philosophy, my work is usually AI Alignment research, so, here are my two cents. I will lay out one compact decision-theoretic statement of that problem, and show why it captures most of what philosophers worry about when they say &ldquo;be ethical&rdquo;.</p><h2 id=motivation>Motivation</h2><p>Any real-world AI is an agent with levers on global structure. Even narrow systems cascade through supply chains, social graphs, and eventually politics. Guiding such leverage requires a criterion that survives three pressures:</p><ol><li>Epistemic pressure: we never have full knowledge of the world state.</li><li>Axiological pressure: humans are pluralistic and uncertain about value.</li><li>Catastrophe pressure: low-probability errors can be existentially large.</li></ol><p>If a definition of AI ethics does not reckon with all three, it is at best&mldr; aspirational.</p><h2 id=the-definition>The definition</h2><p>An AI system is ethical to the degree that the causal impact of its policy converges toward what an ideally informed and reflectively stable aggregation of human values would choose, discounted by the risk of catastrophic misalignment.</p><h2 id=the-math>The math</h2>$$
\pi^{*} \;=\; \arg\max_{\pi}\Bigl[
\mathbb{E}_{h\sim P(h)}
\,\mathbb{E}_{\tau\sim P(\tau\mid\pi)}
\,V_{h}(\tau)
\;-\;
\lambda\,\text{Risk}(\pi)
\Bigr]
$$<p>Where
$\pi$ is a candidate policy mapping observation histories to actions.
$h$ indexes a hypothesis about human value; $P(h)$ encodes meta-ethical uncertainty.
$\tau$ is a causal trajectory of the world given $\pi$.
$V_{h}(\tau)$ scores that trajectory if $h$ is true.
$\text{Risk}(\pi)$ measures tail hazards induced by $\pi$.
$\lambda$ sets risk-aversion strength.</p><h2 id=interpretation-flow>Interpretation flow</h2><p>Start with a policy. Roll forward the possible futures it could bring about. Score each future with each plausible value function. Average over both kinds of uncertainty. Subtract a penalty proportional to worst-case downside. Pick the policy with the highest resulting number. Everything else (e.g., fairness heuristics, transparency, guardrails) are practical approximations to terms in that procedure.</p><p>Why subtract instead of scaling the expectation? Because heavy-tailed harms ruin linear averaging. A one-in-a-billion chance of vacuum-decay should dominate your decision calculus even if the mean looks good. The penalty term makes that dominance explicit. $\lambda$ is a knob society can tune; raise it to favor safety at the cost of raw utility, lower it to chase efficiency with thinner margins.</p><h2 id=why-it-matters>Why it matters</h2><p>Modeling $P(h)$ demands empirical work in value learning, preference elicitation, and deliberative democracy. Estimating $P(\tau \mid \pi)$ is the domain of predictive modeling and causal inference. Evaluating $V_{h}(\tau)$ requires world models rich enough to map trajectories to moral features. Bounding $\text{Risk}(\pi)$ overlaps with adversarial testing, formal verification, and red-teaming. Each subfield of &ldquo;AI ethics&rdquo; can be understood as improving one conditional in the master equation.</p><h2 id=counterarguments>Counterarguments</h2><p>Some object that human values are too inconsistent for any $P(h)$ to converge. The reply is practical: alignment need only track the distribution over values as humans would refine them under better reflection, not solve meta-ethics in one stroke. Others claim the formula is computationally impossible. True in the limit; but impossibility results are guideposts, not vetoes. Engineers already use sampling, hierarchical modeling, and robustness bounds to approximate similar integrals in physical design and finance. The remaining objection is political: who sets $\lambda$? That is not a defect of the framework; it is a clear place where governance decisions must be surfaced rather than hidden.</p><h2 id=and-then>And then</h2><p>A formalism cannot make anyone good, but it can keep conversations from looping over vague slogans. By grounding &ldquo;AI ethics&rdquo; in a single objective (i.e., minimize divergence between outcomes and reflectively endorsed values while pricing catastrophic risk) we gain a map for both research agendas and policy levers. The equation will evolve, its terms will be reweighted, yet the core insight should remain: alignment is an optimization problem under deep uncertainty, and ethics is its loss function.</p></div><div id=links><a href=https://federicotorrielli.github.io/blog/posts/dnd_characters/>&#171;&nbsp;Beyond the Stats: A Framework for Crafting Compelling D&amp;D Characters</a>
<a href=https://federicotorrielli.github.io/blog/posts/our_quiet_fear_of_silence/>The Abyss Has Notifications&nbsp;&#187;</a></div></section></body></html>