<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer"><meta name=description content><meta property="og:title" content="Agi Is Not Coming"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:url" content="https://federicotorrielli.github.io/blog/posts/agi-is-not-coming/"><meta property="og:site_name" content="Federico Torrielli - Blog"><meta property="og:image" content="https://files.catbox.moe/y70xve.webp"><meta property="og:image:alt" content="Agi Is Not Coming"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Agi Is Not Coming"><meta name=twitter:description content><meta name=twitter:image content="https://files.catbox.moe/y70xve.webp"><meta name=twitter:image:alt content="Agi Is Not Coming"><meta property="article:published_time" content="2025-09-01T14:00:00+02:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="AGI"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="Philosophy"><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400&display=swap" rel=stylesheet media=print type=text/css onload='this.media="all"'><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]},loader:{load:["ui/safe"]}}</script><title>Agi Is Not Coming</title><link rel=canonical href=https://federicotorrielli.github.io/blog/posts/agi-is-not-coming/><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;line-height:160%;color:#1d1313;max-width:700px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3{font-family:old standard tt,serif}strong{font-weight:600}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-size:35px}h2{font-size:28px}h3{font-size:22px;margin-top:18px}h1 a,h2 a,h3 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,.date{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content .date{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#links{display:flex;justify-content:space-between;margin:50px 0 0}#links :nth-child(1){margin-right:.5em}#links :nth-child(2){margin-left:.5em}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}@media(prefers-color-scheme:dark){*,#nav h1 a{color:#fdfdfd}body{background:#121212}pre,code{background-color:#262626}#sub-header,.date{color:#bababa}hr{background:#ebebeb}}</style></head><body><section id=nav><h1><a class=h-card href=https://federicotorrielli.github.io/blog/>Federico Torrielli - Blog</a></h1><ul><li><a href=https://federicotorrielli.github.io/blog/>Home</a></li><li><a href=https://github.com/federicotorrielli>GitHub</a></li><li><a href=https://evilscript.eu>Website</a></li></ul></section></body></html><section id=content><h1>Agi Is Not Coming</h1><div id=sub-header>September 2025 · 5 minute read</div><div class=entry-content><p><img src=https://files.catbox.moe/y70xve.webp alt="AGI is not coming"></p><blockquote><p>The question of whether Machines Can Think&mldr; is about as relevant as the question of whether Submarines Can Swim. — Edsger Dijkstra (1984)</p></blockquote><p>We observe systems that demonstrate superhuman aptitude in narrow domains, yet fail at tasks requiring what seems to be trivial common sense or memory. One popular framing suggests the path to AGI is blocked not by a need for more scale, but by a set of &ldquo;engineering problems&rdquo;: we lack persistent memory, robust agentic scaffolding, and effective long-term planning frameworks. The underlying assumption is that the core intelligence, the Large Language Model, is a sufficiently powerful cognitive engine, and we must now simply build the correct chassis around it.</p><p>This framing feels incomplete. It seems to mistake a symptom for the disease. The reason current models lack these capabilities is not an incidental engineering oversight. The limitation is inherent to the architecture itself. The very concept of &ldquo;bolting on&rdquo; memory or agency to a pretrained transformer is a category error, rooted in a misunderstanding of what these models are. We are trying to engineer around a (already flawed) foundational property.</p><p>To clarify this, we need a better distinction than &ldquo;model training&rdquo; versus &ldquo;engineering&rdquo;. The critical distinction is between what I will term <strong>Inscribed Architectures</strong> and <strong>Adaptive Architectures</strong>.</p><p>Current frontier models, from GPT-5 to Claude 4, are Inscribed Architectures. The entirety of their pretraining and fine-tuning process is a monumental effort to inscribe a compressed, high-dimensional representation of their training data into a fixed set of weights, $W$. Inference is the process of navigating this static, inscribed manifold. A prompt is a starting vector, and the autoregressive process is a trajectory through this pre-computed space. The model&rsquo;s &ldquo;knowledge&rdquo; can be formalized as the very geometry of this manifold (and not a set of retrievable facts). RLHF and constitutional AI can&rsquo;t change this; they merely warp the manifold to make certain trajectories more probable and others less so. The context window, even when extended to millions of tokens, it is a temporary, local distortion of the activation space, which vanishes the moment the session ends. The underlying inscription $W$ remains untouched.</p><p>An Adaptive Architecture, by contrast, is one where interaction with the environment produces durable and efficient updates to the model&rsquo;s core parameters $W$. This is not the same as continual fine-tuning, which is computationally expensive and risks catastrophic forgetting. An adaptive process, $W_t -> W_{t+1}$, would be an intrinsic function of the model, operating continuously and locally as it processes new information. This is the architectural property that gives rise to what we call continual learning. A human does not learn to play the saxophone by receiving a refined set of instructions for the next attempt. Each breath, each note, and each error provides a feedback signal that physically alters their neural substrate. Their $W$ is in constant flux.</p><p>The &ldquo;engineering&rdquo; solutions currently being pursued are attempts to simulate properties of adaptive systems using an inscribed core. RAG is a way to project external data into the temporary context window, simulating factual recall without updating the world model. Chain-of-thought prompting forces the model to traverse a longer, more structured trajectory within its inscribed manifold, simulating reasoning without genuine deliberation. These are clever and useful techniques, but they are building scaffolding around a fundamentally static object. The scaffolding can never change the nature of the object itself. It will always be brittle at the interface.</p><p>This reframing helps us understand why progress can feel both miraculous and stagnant. For any task that can be well represented within the inscribed data manifold, performance will be extraordinary. This is interpolation on a cosmic scale. But for tasks that require the construction of new knowledge, the durable accumulation of context, or the revision of core beliefs based on novel evidence, the architecture has no native mechanism. The model cannot learn, in any meaningful sense of the word, because its parameters are frozen. It can only perform.</p><p>The implications for AGI are significant. If AGI is defined as <em>a system that can autonomously learn and adapt across a wide range of domains</em>, then inscribed architectures are a dead end. No amount of scaling or clever scaffolding will bridge this fundamental gap. The resulting system will be a more and more comprehensive map of its training data, but it will never become a map-maker. It is a system that has been given a perfect memory of a library, but no capacity to write a new book.</p><p>This also sharpens our view of alignment. Aligning an inscribed model is a problem of constraining its outputs, of fencing off undesirable regions of its static knowledge manifold. Aligning an adaptive model is a problem of aligning its <em>update rule</em>: the very process by which it modifies its own cognition. The latter is a far more complex and dangerous problem, one that involves understanding the dynamics of a self-modifying mesa-optimizer. We are currently struggling with the former, which should give us pause about the prospect of building the latter.</p><p>Our present focus on engineering modules around a static core may be a necessary research step, but we should not mistake it for the path to AGI.</p><p>What architectural primitives would even support efficient, durable, local weight updates? How can a system learn to modify its own structure without succumbing to instability? These are questions of computer science and neuroscience, and aren&rsquo;t to be answered using software engineering. Until we can move from inscribed to adaptive architectures, AGI will not be &ldquo;right around the corner&rdquo;. We will simply be building ever-more-elaborate puppets, with no ghost in the machine.</p></div><div id=links><a href=https://federicotorrielli.github.io/blog/posts/beyond-ai-bubble/>&#171;&nbsp;Beyond the AI Bubble</a>
<a href=https://federicotorrielli.github.io/blog/posts/bayesian-in-expectation-is-not-a-theorem/>"Bayesian in expectation" is not a theorem (yet): reviewing the argument&nbsp;&#187;</a></div></section></body></html>