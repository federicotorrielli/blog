<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Philosophy on Federico Torrielli - Blog</title><link>https://federicotorrielli.github.io/blog/tags/philosophy/</link><description>Recent content in Philosophy on Federico Torrielli - Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 01 Sep 2025 14:00:00 +0200</lastBuildDate><atom:link href="https://federicotorrielli.github.io/blog/tags/philosophy/index.xml" rel="self" type="application/rss+xml"/><item><title>Agi Is Not Coming</title><link>https://federicotorrielli.github.io/blog/posts/agi-is-not-coming/</link><pubDate>Mon, 01 Sep 2025 14:00:00 +0200</pubDate><guid>https://federicotorrielli.github.io/blog/posts/agi-is-not-coming/</guid><description>&lt;p&gt;&lt;img src="https://files.catbox.moe/y70xve.webp" alt="AGI is not coming"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The question of whether Machines Can Think&amp;hellip; is about as relevant as the question of whether Submarines Can Swim. â€” Edsger Dijkstra (1984)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We observe systems that demonstrate superhuman aptitude in narrow domains, yet fail at tasks requiring what seems to be trivial common sense or memory. One popular framing suggests the path to AGI is blocked not by a need for more scale, but by a set of &amp;ldquo;engineering problems&amp;rdquo;: we lack persistent memory, robust agentic scaffolding, and effective long-term planning frameworks. The underlying assumption is that the core intelligence, the Large Language Model, is a sufficiently powerful cognitive engine, and we must now simply build the correct chassis around it.&lt;/p&gt;</description></item></channel></rss>