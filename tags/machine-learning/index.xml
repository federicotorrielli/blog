<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on Federico Torrielli - Blog</title><link>https://federicotorrielli.github.io/blog/tags/machine-learning/</link><description>Recent content in Machine Learning on Federico Torrielli - Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 08 Sep 2025 11:48:37 +0200</lastBuildDate><atom:link href="https://federicotorrielli.github.io/blog/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>"Bayesian in expectation" is not a theorem (yet): reviewing the argument</title><link>https://federicotorrielli.github.io/blog/posts/bayesian-in-expectation-is-not-a-theorem/</link><pubDate>Mon, 08 Sep 2025 11:48:37 +0200</pubDate><guid>https://federicotorrielli.github.io/blog/posts/bayesian-in-expectation-is-not-a-theorem/</guid><description>&lt;p&gt;&lt;img src="https://files.catbox.moe/e3xvqv.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This analysis represents my current understanding of the technical claims in the referenced paper. The critiques presented here may contain errors in interpretation or technical detail. This is an evolving post that may be revised as I receive feedback or identify mistakes in my reasoning. Readers should consult &lt;a href="https://arxiv.org/abs/2507.11768"&gt;the original paper&lt;/a&gt; and form their own technical judgments. I welcome corrections and constructive discussion of any points raised below.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The paper aims to resolve two conflicting observations about large language models. First, positional encodings violate the exchangeability conditions required for classical Bayesian learning. Second, these models appear to compress data well and sometimes look Bayesian in spirit. The proposed resolution is that transformers are Bayesian in expectation over permutations but not in the realization at a fixed ordering. The paper presents four quantitative results and several empirical confirmations.&lt;/p&gt;</description></item><item><title>Agi Is Not Coming</title><link>https://federicotorrielli.github.io/blog/posts/agi-is-not-coming/</link><pubDate>Mon, 01 Sep 2025 14:00:00 +0200</pubDate><guid>https://federicotorrielli.github.io/blog/posts/agi-is-not-coming/</guid><description>&lt;p&gt;&lt;img src="https://files.catbox.moe/y70xve.webp" alt="AGI is not coming"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The question of whether Machines Can Think&amp;hellip; is about as relevant as the question of whether Submarines Can Swim. â€” Edsger Dijkstra (1984)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We observe systems that demonstrate superhuman aptitude in narrow domains, yet fail at tasks requiring what seems to be trivial common sense or memory. One popular framing suggests the path to AGI is blocked not by a need for more scale, but by a set of &amp;ldquo;engineering problems&amp;rdquo;: we lack persistent memory, robust agentic scaffolding, and effective long-term planning frameworks. The underlying assumption is that the core intelligence, the Large Language Model, is a sufficiently powerful cognitive engine, and we must now simply build the correct chassis around it.&lt;/p&gt;</description></item></channel></rss>